{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone and fetch the index\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "index = pc.Index(PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding model\n",
    "embedding_model = VertexAIEmbeddings(model_name=\"text-embedding-005\")\n",
    "\n",
    "# Initialise vectorstore\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='4ea51373-146b-4e09-883c-27d8559cb887', metadata={'source': 'raw_data/LLMs_Transformers_2405.06640v1.pdf'}, page_content='Efficiency improvements for vanilla transformers have narrowed the capabilities gap between\\nvanilla and linear transformers. The KV cache Pope et al. (2023)greatly narrows the inference\\nefficiency gap between linear and vanilla transformers. RingAttention Liu et al. (2023) allows for\\nvery long context scaling of vanilla attention without approximation.\\nState Space Models.\\nState-space models (SSMs) such as H3 (Dao et al., 2022), Hyena (Poli et al.,\\n2023), and Mamba (Gu & Dao, 2023) are recent alternatives to vanilla transformers, combining the\\nstrengths of convolutional and recurrent models with efficient hardware implementations. Instead\\nof parallelizing training over the sequence, they produce an efficient way to train the sequential\\nRNN. While these models are competitive with vanilla transformers on some tasks, we show that\\nSSMs share the limitations of linear transformers on several in-context learning and long-context\\ntasks.\\nUptraining Linear Transformers.'),\n",
       " Document(id='20cfafb6-de14-4041-a8b4-695a21711e68', metadata={'source': 'raw_data/LLMs_Transformers_2412.07201v1.pdf'}, page_content='4. Challenges and Future Directions\\nTransformer-based language models have been proven to provide eﬀective and signiﬁcant results when applied to\\nbiological sequences, especially for their ability to deﬁne and handle a huge number of context-dependent features.\\nNevertheless, in order to build even more reliable and better performing Transformer models, some issues are still to\\nbe addressed; a huge amount of computational resources is needed to build Transformer models. Thus, the scientiﬁc\\ncommunity is devoting many eﬀorts in order to reduce the computational load both in terms of time and space com-\\nplexity. Also, some features of the models can be customized and particular attention could be paid to overcome the\\ncommon limitation of deep learning model in interpreting and reading the intrinsic meaning of the models. There is\\nstill a large margin of improvement that could be achieved by developing models tailored for speciﬁc contexts as well'),\n",
       " Document(id='a5382229-4a46-4531-b799-c9d0c4424951', metadata={'source': 'raw_data/LLMs_Transformers_2410.05258v1.pdf'}, page_content='Specifically, the “Transformer” models include improvements in RMSNorm [46], SwiGLU [35, 29],\\nand removal of bias.\\nScaling Model Size\\nAs shown in Figure 3a, we train language models with 830M, 1.4B, 2.8B,\\n6.8B, and 13.1B parameters. The models are trained with a sequence length of 2048, and a batch\\nsize of 0.25M tokens. We train models for 40K steps. Detailed hyperparameters are described\\nin Appendix D. The scaling law [18] empirically fits well in this configuration. Figure 3a shows\\nthat DIFF Transformer outperforms Transformer in various model sizes. The results indicate that\\nDIFF Transformer is scalable in terms of parameter count. According to the fitted curves, 6.8B-size\\nDIFF Transformer achieves a validation loss comparable to 11B-size Transformer, requiring only\\n62.2% of parameters. Similarly, 7.8B-size DIFF Transformer matches the performance of 13.1B-size\\nTransformer, requiring only 59.5% of parameters.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if the vectorstore is connected through a similarity search\n",
    "vector_store.similarity_search(\"What are the latest advancements in transformer models?\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the prompt, parser and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate LLM\n",
    "llm = ChatVertexAI(model=\"gemini-2.5-pro-exp-03-25\", temperature=0.2)\n",
    "\n",
    "# Instantiate output parser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the prompt template string\n",
    "prompt_template = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Keep the answer as concise as possible.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Generate the prompt template from the template string\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup retriever and function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_system(question):\n",
    "    # Retrieve relevant docs\n",
    "    docs = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}).invoke(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # Instantiate the chain\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    # Run your existing chain\n",
    "    result = chain.invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "    # Print the answer\n",
    "    print(\"Answer:\\n\", result)\n",
    "\n",
    "    # Print sources\n",
    "    print(\"\\nSources used:\")\n",
    "    seen = set()\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get(\"source\", \"unknown\")\n",
    "        if source not in seen:\n",
    "            print(f\"{i}. {source}\")\n",
    "            seen.add(source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Recent advancements in transformer models include:\n",
      "*   Efficiency improvements such as KV cache for inference and RingAttention for long context scaling.\n",
      "*   Architectural improvements like RMSNorm, SwiGLU, and removal of bias.\n",
      "*   Development of more scalable variants like DIFF Transformer.\n",
      "*   Emergence of alternatives like State-Space Models (SSMs) such as Mamba, Hyena, and H3.\n",
      "\n",
      "Sources used:\n",
      "1. raw_data/LLMs_Transformers_2405.06640v1.pdf\n",
      "2. raw_data/LLMs_Transformers_2412.07201v1.pdf\n",
      "3. raw_data/LLMs_Transformers_2410.05258v1.pdf\n",
      "4. raw_data/LLMs_Transformers_2405.04515v2.pdf\n",
      "5. raw_data/LLMs_Transformers_2108.00104v1.pdf\n"
     ]
    }
   ],
   "source": [
    "invoke_system(\"What are the latest advancements in transformer models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " A Transformer is a machine learning architecture introduced in 2017, initially for machine translation, which has become a major breakthrough in AI and natural language processing (NLP). It uses attention mechanisms and multi-layer perceptrons (MLPs) to understand relationships within sequences, like words in a sentence. Formally, it's defined as a function mapping an input sequence to an output sequence through layers containing multi-head attention and position-wise feed-forward networks.\n",
      "\n",
      "Sources used:\n",
      "1. raw_data/LLMs_Transformers_2403.00807v1.pdf\n",
      "2. raw_data/LLMs_Transformers_2402.09748v1.pdf\n",
      "3. raw_data/LLMs_Transformers_2412.07201v1.pdf\n",
      "5. raw_data/LLMs_Transformers_2410.14706v2.pdf\n",
      "Answer:\n",
      " NLP is an interdisciplinary field involving linguistics, computer science, and mathematics that aims to enable computers to understand, process, and generate natural language text or speech.\n",
      "\n",
      "Sources used:\n",
      "1. raw_data/LLMs_Transformers_2503.02435v1.pdf\n",
      "5. raw_data/LLMs_Transformers_2412.07201v1.pdf\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"Ask a question (or 'exit'): \")\n",
    "    if question.lower() == \"exit\":\n",
    "        break\n",
    "    invoke_system(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smart-qna-gUrRHHow-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
